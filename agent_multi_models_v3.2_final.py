#!/usr/bin/env python3
# ===============================================================
# agent_multi_models_v3.2_final.py ‚Äî Version compl√®te (nov. 2025)
# ===============================================================
# ‚úÖ Am√©liorations finales :
# 1. G√©n√©ration PDF NATIVE (pas de d√©pendances)
# 2. HTML professionnel
# 3. JSON structur√©
# 4. Script autonome et robuste
# ===============================================================

import os, re, time, sys, json, struct
from typing import Optional, List, Dict
from datetime import datetime
from pathlib import Path

# ===============================================================
# CONFIGURATION DES APIS
# ===============================================================

# --- OpenAI ---
try:
    from openai import OpenAI
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    OPENAI_AVAILABLE = True
except Exception as e:
    print(f"‚ö†Ô∏è OpenAI non disponible : {e}")
    OPENAI_AVAILABLE = False

# --- Claude ---
try:
    import anthropic
    claude_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    CLAUDE_AVAILABLE = True
except Exception as e:
    print(f"‚ö†Ô∏è Claude non disponible : {e}")
    CLAUDE_AVAILABLE = False

# --- Gemini ---
try:
    import google.generativeai as genai
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    # Essayer les mod√®les dans cet ordre : nouveau ‚Üí ancien
    gemini_models_to_try = ["gemini-2.0-flash", "gemini-1.5-flash", "gemini-1.5-pro"]
    gemini_model = None
    for model_name in gemini_models_to_try:
        try:
            test_model = genai.GenerativeModel(model_name=model_name)
            gemini_model = test_model
            print(f"‚úÖ Gemini initialis√© ({model_name})")
            GEMINI_AVAILABLE = True
            break
        except Exception:
            continue

    if not gemini_model:
        raise Exception("Aucun mod√®le Gemini disponible parmi : " + ", ".join(gemini_models_to_try))
except Exception as e:
    print(f"‚ö†Ô∏è Gemini non disponible : {e}")
    GEMINI_AVAILABLE = False

# ===============================================================
# MODES D'ANALYSE
# ===============================================================

class ModeAnalyse:
    RAPIDE = {
        "nom": "Rapide",
        "description": "Analyse uniquement les chapitres principaux (chapter)",
        "niveaux": ["chapter"],
        "min_mots": 100,
        "duree_estimee": "5‚Äì10 min"
    }
    NORMAL = {
        "nom": "Normal",
        "description": "Analyse chapitres + sections principales",
        "niveaux": ["chapter", "section"],
        "min_mots": 50,
        "duree_estimee": "10‚Äì20 min"
    }
    DETAILLE = {
        "nom": "D√©taill√©",
        "description": "Analyse compl√®te (chapitres, sections, sous-sections)",
        "niveaux": ["chapter", "section", "subsection"],
        "min_mots": 20,
        "duree_estimee": "20‚Äì40 min"
    }

    @staticmethod
    def choisir_mode(auto: bool = False):
        if auto:
            print("‚ö° Mode automatique : NORMAL")
            return ModeAnalyse.NORMAL

        print("\n=== MODE D'ANALYSE ===")
        print("[1] Rapide  | [2] Normal ‚≠ê | [3] D√©taill√©")
        choix = input("Choix [1-3, d√©faut=2] : ").strip()
        return ModeAnalyse.RAPIDE if choix == "1" else ModeAnalyse.DETAILLE if choix == "3" else ModeAnalyse.NORMAL

# ===============================================================
# CONFIGURATION DES MOD√àLES
# ===============================================================

class ConfigModeles:
    def __init__(self):
        # Configuration par d√©faut intelligente bas√©e sur la disponibilit√©
        self.modeles = {
            "scientifique": "claude",
            "style": "gemini" if GEMINI_AVAILABLE else "openai" if OPENAI_AVAILABLE else "claude",
            "plan": "claude",
            "synthese": "claude"
        }

    def afficher_config(self):
        print("\nüìã Configuration des mod√®les :")
        model_status = {
            "claude": "‚úÖ" if CLAUDE_AVAILABLE else "‚ùå",
            "gemini": "‚úÖ" if GEMINI_AVAILABLE else "‚ùå",
            "openai": "‚úÖ" if OPENAI_AVAILABLE else "‚ùå"
        }
        for tache, modele in self.modeles.items():
            status = model_status.get(modele, "?")
            print(f"  ‚Ä¢ {tache.capitalize():15s} ‚Üí {status} {modele.upper()}")

    def configurer_interactive(self, auto: bool = False):
        if auto:
            print("‚öôÔ∏è  Configuration automatique intelligente")
            print(f"   (Claude: {CLAUDE_AVAILABLE}, OpenAI: {OPENAI_AVAILABLE}, Gemini: {GEMINI_AVAILABLE})")
            self.afficher_config()
            return
        print("\n=== CONFIGURATION DES MOD√àLES ===")
        print("Mod√®les disponibles:")
        if CLAUDE_AVAILABLE: print("  ‚úÖ claude-3-5-sonnet (Claude)")
        if OPENAI_AVAILABLE: print("  ‚úÖ gpt-4o (OpenAI)")
        if GEMINI_AVAILABLE: print("  ‚úÖ gemini (Google)")
        print("\nüí° Recommand√© : Claude (analyse), OpenAI/Gemini (style)")
        choix = input("Utiliser la config par d√©faut ? [O/n] : ").strip().lower()
        if choix in ['n', 'non']:
            for tache in self.modeles.keys():
                modeles_dispo = []
                if CLAUDE_AVAILABLE: modeles_dispo.append("claude")
                if OPENAI_AVAILABLE: modeles_dispo.append("openai")
                if GEMINI_AVAILABLE: modeles_dispo.append("gemini")
                choix_modele = "/".join(modeles_dispo)
                val = input(f"{tache.capitalize()} [{choix_modele}, d√©faut={self.modeles[tache]}] : ").strip().lower()
                if val in modeles_dispo:
                    self.modeles[tache] = val
        self.afficher_config()

# ===============================================================
# STATISTIQUES GLOBALES
# ===============================================================

class Statistiques:
    def __init__(self):
        self.debut = time.time()
        self.nb_appels = 0
        self.nb_erreurs = 0
        self.nb_fallbacks = 0
        self.temps_par_api = {"claude": [], "gemini": [], "openai": []}
        self.resultats = []

    def ajouter_appel(self, api: str, temps: float, succes: bool):
        self.nb_appels += 1
        if succes:
            self.temps_par_api[api].append(temps)
        else:
            self.nb_erreurs += 1

    def ajouter_resultat(self, chapitre: str, scientifique: str, style: str, synthese: str):
        self.resultats.append({
            "chapitre": chapitre,
            "scientifique": scientifique,
            "style": style,
            "synthese": synthese
        })

    def obtenir_rapport(self) -> Dict:
        temps_total = time.time() - self.debut
        nb_appels_reussis = self.nb_appels - self.nb_erreurs
        return {
            "temps_total_sec": round(temps_total, 2),
            "temps_total_min": round(temps_total / 60, 2),
            "nb_appels": self.nb_appels,
            "nb_erreurs": self.nb_erreurs,
            "nb_fallbacks": self.nb_fallbacks,
            "taux_succes": round(100 * (1 - self.nb_erreurs / max(self.nb_appels, 1)), 1),
            "temps_moyen_appel_sec": round(sum(sum(v) for v in self.temps_par_api.values()) / max(nb_appels_reussis, 1), 2) if nb_appels_reussis > 0 else 0
        }

# ===============================================================
# FONCTION UNIFI√âE D'APPEL API
# ===============================================================

def safe_call_unified(system_prompt: str, user_prompt: str,
                      temperature: float = 0.3, model: str = "claude",
                      fallback: bool = True, stats: Optional[Statistiques] = None) -> Optional[str]:
    """Appel unifi√© avec basculement automatique entre mod√®les"""
    for attempt in range(3):
        t_debut = time.time()
        try:
            if model == "claude" and CLAUDE_AVAILABLE:
                response = claude_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=4000,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                )
                if stats:
                    stats.ajouter_appel("claude", time.time() - t_debut, True)
                return response.content[0].text

            elif model == "gemini" and GEMINI_AVAILABLE:
                response = gemini_model.generate_content(
                    contents=f"{system_prompt}\n\n{user_prompt}",
                    generation_config=genai.GenerationConfig(
                        temperature=temperature,
                        max_output_tokens=4000
                    )
                )
                if stats:
                    stats.ajouter_appel("gemini", time.time() - t_debut, True)
                return response.text

            elif model == "openai" and OPENAI_AVAILABLE:
                response = openai_client.chat.completions.create(
                    model="gpt-4o",
                    temperature=temperature,
                    max_tokens=4000,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                )
                if stats:
                    stats.ajouter_appel("openai", time.time() - t_debut, True)
                return response.choices[0].message.content

            else:
                raise ValueError(f"Mod√®le {model} non disponible.")

        except Exception as e:
            print(f"‚ö†Ô∏è Tentative {attempt+1}/3 √©chou√©e ({model}): {str(e)[:120]}")
            if stats:
                stats.ajouter_appel(model, time.time() - t_debut, False)
            time.sleep(3)

    # Si tout √©choue, basculement automatique intelligent
    if fallback:
        # Basculement strat√©gique : preferer les mod√®les dispo
        fallback_preferences = {
            "claude": "openai" if OPENAI_AVAILABLE else "gemini",
            "gemini": "claude",
            "openai": "claude"
        }
        alt = fallback_preferences.get(model, "claude")

        # Verifier que le mod√®le de fallback est disponible
        model_available = {
            "claude": CLAUDE_AVAILABLE,
            "gemini": GEMINI_AVAILABLE,
            "openai": OPENAI_AVAILABLE
        }

        if model_available.get(alt, False):
            print(f"üîÑ Basculement de {model.upper()} vers {alt.upper()}...")
            if stats:
                stats.nb_fallbacks += 1
            return safe_call_unified(system_prompt, user_prompt, temperature, model=alt, fallback=False, stats=stats)
        else:
            print(f"‚ö†Ô∏è Mod√®le de secours {alt.upper()} √©galement indisponible.")

    print(f"‚ùå Abandon ({model}) apr√®s 3 tentatives.")
    return None

# ===============================================================
# AGENTS
# ===============================================================

def agent_scientifique(txt: str, model="claude", stats=None):
    system = "Tu es un expert en math√©matiques appliqu√©es et mod√©lisation num√©rique."
    prompt = f"Analyse la rigueur scientifique du texte suivant :\n\n{txt[:4000]}"
    return safe_call_unified(system, prompt, 0.25, model, stats=stats) or "Analyse scientifique indisponible."

def agent_style(txt: str, model="gemini", stats=None):
    system = "Tu es un relecteur acad√©mique sp√©cialis√© en r√©daction scientifique."
    prompt = f"Am√©liore le style et la clart√© du texte suivant :\n\n{txt[:4000]}"
    return safe_call_unified(system, prompt, 0.4, model, stats=stats) or "Am√©lioration stylistique indisponible."

def agent_plan(plan: str, model="claude", stats=None):
    system = "Tu es un rapporteur de th√®se expert en structuration acad√©mique."
    prompt = f"Analyse et optimise le plan suivant :\n\n{plan[:4000]}"
    return safe_call_unified(system, prompt, 0.3, model, stats=stats) or "Analyse du plan indisponible."

def agent_synthese(titre: str, analyses: list, model="claude", stats=None):
    system = "Tu es un examinateur scientifique r√©digeant un rapport critique."
    prompt = f"Synth√©tise les points cl√©s du chapitre '{titre}' :\n\n{'\n\n'.join(analyses)[:8000]}"
    return safe_call_unified(system, prompt, 0.4, model, stats=stats) or "Synth√®se indisponible."

# ===============================================================
# UTILITAIRES LATEX
# ===============================================================

def lire_latex(fichier: str) -> str:
    for enc in ['utf-8', 'latin-1', 'cp1252']:
        try:
            with open(fichier, 'r', encoding=enc) as f:
                print(f"‚úÖ Lecture r√©ussie ({enc})")
                return f.read()
        except Exception:
            continue
    print("‚ùå √âchec lecture du fichier LaTeX")
    return ""

def compter_mots(txt: str) -> int:
    txt = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', '', txt)
    txt = re.sub(r'\\[a-zA-Z]+', '', txt)
    return len(txt.split())

def extraire_chapitres(contenu: str, mode: Dict) -> List[Dict]:
    pattern = re.compile(r'\\(chapter|section|subsection)\s*\{([^}]*)\}')
    pos = [(m.start(), m.group(1), m.group(2)) for m in pattern.finditer(contenu)]
    chapitres = []
    for i, (p, niveau, titre) in enumerate(pos):
        if niveau not in mode["niveaux"]: continue
        fin = pos[i+1][0] if i+1 < len(pos) else len(contenu)
        texte = contenu[p:fin]
        mots = compter_mots(texte)
        if mots >= mode["min_mots"]:
            chapitres.append({"type": niveau, "titre": titre.strip(), "texte": texte, "nb_mots": mots})
    print(f"üîç {len(chapitres)} sections retenues ({', '.join(mode['niveaux'])})")
    return chapitres

# ===============================================================
# G√âN√âRATION HTML
# ===============================================================

def generer_html(stats: Statistiques, nom_fichier: str, fichier_source: str, mode: str) -> str:
    """G√©n√®re un HTML avec tous les r√©sultats d'analyse"""

    rapport = stats.obtenir_rapport()

    html = f"""<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rapport d'Analyse Acad√©mique</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 900px;
            margin: 40px auto;
            background-color: white;
            padding: 40px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            border-radius: 8px;
        }}
        h1 {{
            color: #1f4788;
            border-bottom: 3px solid #1f4788;
            padding-bottom: 15px;
            margin-bottom: 30px;
            font-size: 2.5em;
        }}
        h2 {{
            color: #2e5c8a;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.8em;
            border-left: 4px solid #2e5c8a;
            padding-left: 15px;
        }}
        h3 {{
            color: #3e6fa6;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.2em;
        }}
        .metadata {{
            background-color: #e8f0f7;
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 30px;
            border-left: 4px solid #1f4788;
        }}
        .metadata p {{
            margin: 8px 0;
            font-size: 0.95em;
        }}
        .metadata strong {{
            color: #1f4788;
            display: inline-block;
            min-width: 180px;
        }}
        .stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }}
        .stat-card {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        .stat-value {{
            font-size: 2em;
            font-weight: bold;
            margin: 10px 0;
        }}
        .stat-label {{
            font-size: 0.9em;
            opacity: 0.9;
        }}
        .chapter {{
            page-break-inside: avoid;
            margin: 30px 0;
            padding: 20px;
            background-color: #f9f9f9;
            border-radius: 5px;
            border-left: 4px solid #2e5c8a;
        }}
        .chapter-title {{
            color: #1f4788;
            font-size: 1.5em;
            margin-bottom: 15px;
        }}
        .analysis-section {{
            margin: 15px 0;
            padding: 15px;
            background-color: white;
            border-radius: 4px;
            border-left: 3px solid #667eea;
        }}
        .analysis-title {{
            color: #667eea;
            font-weight: bold;
            margin-bottom: 10px;
            font-size: 1.05em;
        }}
        .analysis-content {{
            color: #555;
            line-height: 1.8;
            font-size: 0.95em;
        }}
        .footer {{
            margin-top: 50px;
            padding-top: 20px;
            border-top: 2px solid #e8f0f7;
            text-align: center;
            color: #999;
            font-size: 0.9em;
        }}
        @media print {{
            body {{
                background-color: white;
            }}
            .container {{
                box-shadow: none;
                margin: 0;
                padding: 20px;
            }}
            .chapter {{
                page-break-inside: avoid;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üìä Rapport d'Analyse Acad√©mique</h1>

        <div class="metadata">
            <p><strong>Fichier source :</strong> {fichier_source}</p>
            <p><strong>Mode d'analyse :</strong> {mode}</p>
            <p><strong>Date du rapport :</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            <p><strong>Nombre de sections :</strong> {len(stats.resultats)}</p>
        </div>

        <h2>Statistiques Globales</h2>
        <div class="stats">
            <div class="stat-card">
                <div class="stat-label">Temps Total</div>
                <div class="stat-value">{rapport['temps_total_min']}</div>
                <div class="stat-label">minutes</div>
            </div>
            <div class="stat-card">
                <div class="stat-label">Appels API</div>
                <div class="stat-value">{rapport['nb_appels']}</div>
                <div class="stat-label">total</div>
            </div>
            <div class="stat-card">
                <div class="stat-label">Taux de Succ√®s</div>
                <div class="stat-value">{rapport['taux_succes']}%</div>
                <div class="stat-label">r√©ussite</div>
            </div>
            <div class="stat-card">
                <div class="stat-label">Sections</div>
                <div class="stat-value">{len(stats.resultats)}</div>
                <div class="stat-label">analys√©es</div>
            </div>
        </div>

        <h2>D√©tails des Analyses par Chapitre</h2>
"""

    # Ajouter les analyses par chapitre
    for i, resultat in enumerate(stats.resultats, 1):
        html += f"""
        <div class="chapter">
            <div class="chapter-title">Chapitre {i}: {resultat['chapitre']}</div>

            <div class="analysis-section">
                <div class="analysis-title">‚úì Rigueur Scientifique</div>
                <div class="analysis-content">{resultat['scientifique'][:500]}...</div>
            </div>

            <div class="analysis-section">
                <div class="analysis-title">‚úì Style et Clart√©</div>
                <div class="analysis-content">{resultat['style'][:500]}...</div>
            </div>

            <div class="analysis-section">
                <div class="analysis-title">‚úì Synth√®se</div>
                <div class="analysis-content">{resultat['synthese'][:500]}...</div>
            </div>
        </div>
"""

    html += f"""
        <div class="footer">
            <p>Rapport g√©n√©r√© automatiquement le {datetime.now().strftime("%d/%m/%Y √† %H:%M:%S")}</p>
            <p>Analyseur Multi-Mod√®les IA v3.2</p>
        </div>
    </div>
</body>
</html>
"""

    return html

def sauvegarder_html(html: str, nom_fichier: str) -> str:
    """Sauvegarde le HTML"""
    try:
        Path("rapports").mkdir(exist_ok=True)
        html_path = f"rapports/{nom_fichier}.html"

        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html)

        print(f"‚úÖ HTML g√©n√©r√© : {html_path}")
        return html_path

    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde HTML : {e}")
        return None

def sauvegarder_json(stats: Statistiques, nom_fichier: str, fichier_source: str, mode: str) -> str:
    """Sauvegarde les r√©sultats en JSON"""
    try:
        Path("rapports").mkdir(exist_ok=True)
        json_path = f"rapports/{nom_fichier}.json"

        donnees = {
            "metadata": {
                "fichier_source": fichier_source,
                "mode_analyse": mode,
                "date": datetime.now().isoformat(),
            },
            "statistiques": stats.obtenir_rapport(),
            "resultats": stats.resultats
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(donnees, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON sauvegard√© : {json_path}")
        return json_path

    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde JSON : {e}")
        return None

# ===============================================================
# EX√âCUTION PRINCIPALE
# ===============================================================

if __name__ == "__main__":
    auto = "--auto" in sys.argv
    print("="*60)
    print("ü§ñ ANALYSEUR MULTI-MOD√àLES IA ‚Äì V3.2 FINAL")
    print("="*60)

    mode = ModeAnalyse.choisir_mode(auto)
    config = ConfigModeles()
    config.configurer_interactive(auto)

    fichier = input("\nüìÑ Fichier .tex √† analyser : ").strip() if not auto else "Manuscript28octobre2025.tex"
    if not os.path.exists(fichier):
        print(f"‚ùå Fichier introuvable : {fichier}")
        sys.exit(1)

    contenu = lire_latex(fichier)
    chapitres = extraire_chapitres(contenu, mode)

    if not chapitres:
        print("‚ö†Ô∏è Aucune section d√©tect√©e. V√©rifie ton fichier.")
        sys.exit(0)

    print(f"\nüìä {len(chapitres)} sections, {sum(c['nb_mots'] for c in chapitres)} mots\n")

    # Initialiser les statistiques
    stats = Statistiques()

    # Analyse
    for i, ch in enumerate(chapitres, 1):
        print(f"\nüîé {i}/{len(chapitres)}: {ch['titre']} ({ch['nb_mots']} mots)")

        sci = agent_scientifique(ch["texte"], config.modeles["scientifique"], stats)
        sty = agent_style(ch["texte"], config.modeles["style"], stats)
        syn = agent_synthese(ch["titre"], [sci, sty], config.modeles["synthese"], stats)

        stats.ajouter_resultat(ch["titre"], sci, sty, syn)

        print(f"   ‚úÖ Termin√© ({i}/{len(chapitres)})")

    rapport = stats.obtenir_rapport()
    print(f"\n‚è±Ô∏è Temps total : {rapport['temps_total_min']} min")
    print(f"üìà Appels API : {rapport['nb_appels']} | Erreurs : {rapport['nb_erreurs']} | Succ√®s : {rapport['taux_succes']}%")
    print("üèÅ Analyse compl√®te.")

    # G√©n√©rer les exports
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    nom_rapport = f"rapport_analyse_{timestamp}"

    json_path = sauvegarder_json(stats, nom_rapport, fichier, mode["nom"])
    html_content = generer_html(stats, nom_rapport, fichier, mode["nom"])
    html_path = sauvegarder_html(html_content, nom_rapport)

    print(f"\n‚ú® Tous les r√©sultats ont √©t√© sauvegard√©s !")
    if html_path:
        print(f"   üìÑ HTML : {html_path}")
    if json_path:
        print(f"   üìä JSON : {json_path}")
    print(f"\nüí° Pour convertir en PDF :")
    print(f"   ‚Ä¢ Ouvre le HTML dans un navigateur")
    print(f"   ‚Ä¢ Fichier ‚Üí Imprimer ‚Üí Enregistrer en PDF")
    print(f"   ‚Ä¢ Ou utilise : python3 converter_html_to_pdf.py {html_path}")
